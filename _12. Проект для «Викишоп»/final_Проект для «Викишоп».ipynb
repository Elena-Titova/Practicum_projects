{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Отображать все строки\n",
    "pd.set_option('display.max_columns', None)  # Отображать все столбцы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>\"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>The Mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Don't mean to bother you \\n\\nI see that you're...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               text  toxic\n",
       "0            0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1            1  D'aww! He matches this background colour I'm s...      0\n",
       "2            2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3            3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4            4  You, sir, are my hero. Any chance you remember...      0\n",
       "5            5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6            6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7            7  Your vandalism to the Matt Shirvington article...      0\n",
       "8            8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9            9  alignment on this subject and which are contra...      0\n",
       "10          10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0\n",
       "11          11  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0\n",
       "12          12  Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
       "13          13  Before you start throwing accusations and warn...      0\n",
       "14          14  Oh, and the girl above started her arguments w...      0\n",
       "15          15  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...      0\n",
       "16          16  Bye! \\n\\nDon't look, come or think of comming ...      1\n",
       "17          17   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski      0\n",
       "18          18  The Mitsurugi point made no sense - why not ar...      0\n",
       "19          19  Don't mean to bother you \\n\\nI see that you're...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "toxic_comments = pd.read_csv('/datasets/toxic_comments.csv') #чтение файла /datasets/toxic_comments.csv\n",
    "display (toxic_comments.head(20)) #вывод первых строк\n",
    "print (toxic_comments.info()) #вывод общей информации о DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый вывод - в таблице нет пустых значений.\n",
    "Проверим наличие явных дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "print('Количество дубликатов:', toxic_comments.duplicated(subset=['text', 'toxic']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В курсе приводятся примеры лемматизации для русского языка. В некоторых источниках указано, что если использовать для английского текста TfidfVectorizer, то токенизация, стоп-слова и нормализация уже встроены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 127433/127433 [14:19<00:00, 148.25it/s]\n",
      "100%|██████████| 31859/31859 [03:42<00:00, 143.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Разделим данные на признаки и целевой признак\n",
    "texts = toxic_comments['text']\n",
    "target = toxic_comments['toxic']\n",
    "\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Функция очистки текста\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Удалим все символы, кроме латинских букв\n",
    "    return text\n",
    "\n",
    "# Очистим текст\n",
    "X_train_cleaned = X_train.apply(clean_text)\n",
    "X_test_cleaned = X_test.apply(clean_text)\n",
    "\n",
    "\n",
    "# Лемматизация\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words])\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "X_train_lemmatized = X_train_cleaned.progress_apply(lemmatize_text)\n",
    "X_test_lemmatized = X_test_cleaned.progress_apply(lemmatize_text)\n",
    "\n",
    "# Векторизация\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train_lemmatized)\n",
    "X_test_vectorized = vectorizer.transform(X_test_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск лучших гиперпараметров и сравнение моделей по F1:\n",
      "Logistic Regression:\n",
      "  Лучшая F1: 0.758\n",
      "  Лучшие параметры: {'C': 5, 'penalty': 'l2'}\n",
      "\n",
      "Linear SVC:\n",
      "  Лучшая F1: 0.736\n",
      "  Лучшие параметры: {'C': 5}\n",
      "\n",
      "Лучшая модель: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Список моделей и гиперпараметров\n",
    "models_and_params = {\n",
    "    \"Logistic Regression\": (\n",
    "        LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced', solver='liblinear'),\n",
    "        {'C': [5, 10, 15], 'penalty': ['l1', 'l2']}\n",
    "    ),\n",
    "    \"Linear SVC\": (\n",
    "        LinearSVC(random_state=42, max_iter=5000, class_weight='balanced'),\n",
    "        {'C': [5, 10, 15]}\n",
    "    )\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Поиск лучших гиперпараметров и сравнение моделей по F1:\")\n",
    "\n",
    "for name, (model, param_grid) in models_and_params.items():\n",
    "    grid = GridSearchCV(model, param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "    grid.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    best_f1 = grid.best_score_\n",
    "    best_params = grid.best_params_\n",
    "    cv_results[name] = (best_f1, best_params)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Лучшая F1: {round(best_f1, 3)}\")\n",
    "    print(f\"  Лучшие параметры: {best_params}\\n\")\n",
    "\n",
    "# Определяем лучшую модель\n",
    "best_model_name = max(cv_results, key=lambda name: cv_results[name][0])\n",
    "best_f1, best_params = cv_results[best_model_name]\n",
    "\n",
    "print(f\"Лучшая модель: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1-мера на тестовой выборке для Logistic Regression: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Финальное тестирование лучшей модели\n",
    "best_model_class, param_grid = models_and_params[best_model_name]\n",
    "best_params = cv_results[best_model_name][1]\n",
    "\n",
    "# Обучаем модель с лучшими параметрами на всём тренировочном наборе\n",
    "final_model = best_model_class.set_params(**best_params)\n",
    "final_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Предсказания и F1 на тестовой выборке\n",
    "y_pred = final_model.predict(X_test_vectorized)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"\\nF1-мера на тестовой выборке для {best_model_name}: {round(f1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прекрасно! По ТЗ необходимо было достичь показателя F1 не менее 0.75.\n",
    "F1-мера на тестовой выборке для Logistic Regression: 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. \n",
    "В проекте требовалось создать инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "В рамках проекта была выполнена предобработка данных, обучены модели с целью классифицировать комментарии на позитивные и негативные. \n",
    "\n",
    "По ТЗ необходимо было достичь показателя F1 не менее 0.75.\n",
    "F1-мера на тестовой выборке для Logistic Regression: 0.76"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 50,
    "start_time": "2025-05-01T19:28:59.306Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-01T19:28:59.357Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-01T19:28:59.358Z"
   },
   {
    "duration": 871,
    "start_time": "2025-05-01T19:29:45.309Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-01T19:29:46.182Z"
   },
   {
    "duration": 1003,
    "start_time": "2025-05-01T19:29:46.188Z"
   },
   {
    "duration": 230,
    "start_time": "2025-05-01T19:32:25.051Z"
   },
   {
    "duration": 196,
    "start_time": "2025-05-01T19:33:50.934Z"
   },
   {
    "duration": 920,
    "start_time": "2025-05-01T19:34:32.401Z"
   },
   {
    "duration": 1384,
    "start_time": "2025-05-01T20:03:41.598Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-01T20:03:42.984Z"
   },
   {
    "duration": 995,
    "start_time": "2025-05-01T20:03:42.989Z"
   },
   {
    "duration": 67,
    "start_time": "2025-05-01T20:03:43.986Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-01T20:03:44.055Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-01T20:03:44.056Z"
   },
   {
    "duration": 1382,
    "start_time": "2025-05-01T20:04:02.891Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-01T20:04:04.275Z"
   },
   {
    "duration": 1067,
    "start_time": "2025-05-01T20:04:04.280Z"
   },
   {
    "duration": 240,
    "start_time": "2025-05-01T20:04:05.350Z"
   },
   {
    "duration": 8884,
    "start_time": "2025-05-01T20:04:05.591Z"
   },
   {
    "duration": 45657,
    "start_time": "2025-05-01T20:06:15.093Z"
   },
   {
    "duration": 1453,
    "start_time": "2025-05-01T20:08:24.940Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-01T20:08:26.395Z"
   },
   {
    "duration": 1035,
    "start_time": "2025-05-01T20:08:26.401Z"
   },
   {
    "duration": 257,
    "start_time": "2025-05-01T20:08:27.438Z"
   },
   {
    "duration": 9245,
    "start_time": "2025-05-01T20:08:27.697Z"
   },
   {
    "duration": 771745,
    "start_time": "2025-05-01T20:08:36.944Z"
   },
   {
    "duration": 1495,
    "start_time": "2025-05-01T20:30:05.713Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-01T20:30:07.209Z"
   },
   {
    "duration": 1045,
    "start_time": "2025-05-01T20:30:07.214Z"
   },
   {
    "duration": 242,
    "start_time": "2025-05-01T20:30:08.261Z"
   },
   {
    "duration": 8973,
    "start_time": "2025-05-01T20:30:08.506Z"
   },
   {
    "duration": 1461,
    "start_time": "2025-05-01T20:30:27.858Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-01T20:30:29.321Z"
   },
   {
    "duration": 1063,
    "start_time": "2025-05-01T20:30:29.326Z"
   },
   {
    "duration": 253,
    "start_time": "2025-05-01T20:30:30.391Z"
   },
   {
    "duration": 9540,
    "start_time": "2025-05-01T20:30:30.647Z"
   },
   {
    "duration": 1608,
    "start_time": "2025-05-01T20:39:24.648Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-01T20:39:26.257Z"
   },
   {
    "duration": 1144,
    "start_time": "2025-05-01T20:39:26.262Z"
   },
   {
    "duration": 267,
    "start_time": "2025-05-01T20:39:27.408Z"
   },
   {
    "duration": 9221,
    "start_time": "2025-05-01T20:39:27.677Z"
   },
   {
    "duration": 756703,
    "start_time": "2025-05-01T21:06:59.145Z"
   },
   {
    "duration": 1383,
    "start_time": "2025-05-01T21:20:12.386Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-01T21:20:13.771Z"
   },
   {
    "duration": 1073,
    "start_time": "2025-05-01T21:20:13.775Z"
   },
   {
    "duration": 246,
    "start_time": "2025-05-01T21:20:14.850Z"
   },
   {
    "duration": 8862,
    "start_time": "2025-05-01T21:20:15.098Z"
   },
   {
    "duration": 740615,
    "start_time": "2025-05-01T21:20:23.962Z"
   },
   {
    "duration": 1795,
    "start_time": "2025-05-02T20:23:28.669Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-02T20:23:30.466Z"
   },
   {
    "duration": 1038,
    "start_time": "2025-05-02T20:23:30.471Z"
   },
   {
    "duration": 267,
    "start_time": "2025-05-02T20:23:31.510Z"
   },
   {
    "duration": 1177601,
    "start_time": "2025-05-02T20:23:31.780Z"
   },
   {
    "duration": 1058814,
    "start_time": "2025-05-02T20:43:09.382Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-02T21:00:48.198Z"
   },
   {
    "duration": 1499,
    "start_time": "2025-05-02T21:05:32.877Z"
   },
   {
    "duration": 2,
    "start_time": "2025-05-02T21:05:34.378Z"
   },
   {
    "duration": 1490,
    "start_time": "2025-05-02T21:05:34.381Z"
   },
   {
    "duration": 426,
    "start_time": "2025-05-02T21:05:35.873Z"
   },
   {
    "duration": 1075558,
    "start_time": "2025-05-02T21:05:36.301Z"
   },
   {
    "duration": 380538,
    "start_time": "2025-05-02T21:23:31.860Z"
   },
   {
    "duration": 15,
    "start_time": "2025-05-02T21:29:52.399Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-02T21:41:17.882Z"
   },
   {
    "duration": 11,
    "start_time": "2025-05-02T21:41:24.276Z"
   },
   {
    "duration": 198767,
    "start_time": "2025-05-02T21:41:38.076Z"
   },
   {
    "duration": 17,
    "start_time": "2025-05-02T21:44:56.845Z"
   },
   {
    "duration": 444490,
    "start_time": "2025-05-02T21:46:18.773Z"
   },
   {
    "duration": 808920,
    "start_time": "2025-05-02T21:54:00.737Z"
   },
   {
    "duration": 16,
    "start_time": "2025-05-02T22:07:29.659Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-02T22:22:28.167Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-02T22:22:28.168Z"
   },
   {
    "duration": 730020,
    "start_time": "2025-05-02T22:22:29.776Z"
   },
   {
    "duration": 291069,
    "start_time": "2025-05-02T22:34:39.798Z"
   },
   {
    "duration": 1391,
    "start_time": "2025-05-02T22:40:08.467Z"
   },
   {
    "duration": 2,
    "start_time": "2025-05-02T22:40:09.860Z"
   },
   {
    "duration": 993,
    "start_time": "2025-05-02T22:40:09.863Z"
   },
   {
    "duration": 231,
    "start_time": "2025-05-02T22:40:10.857Z"
   },
   {
    "duration": 1090462,
    "start_time": "2025-05-02T22:40:11.090Z"
   },
   {
    "duration": 783666,
    "start_time": "2025-05-02T22:58:21.553Z"
   },
   {
    "duration": 24648,
    "start_time": "2025-05-02T23:11:25.220Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
